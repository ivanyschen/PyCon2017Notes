{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample data to store into dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "keys = 'guido sarah barry rachel tim'.split()\n",
    "val1 = 'blue orange green yellow red'.split()\n",
    "val2 = 'austin dallas tuscon reno portland'.split()\n",
    "val3 = 'apple banana orange pear peach'.split()\n",
    "hashes = list(map(abs, map(hash, keys)))\n",
    "entries = list(zip(hashes, keys, val1))\n",
    "comb_entries = list(zip(hashes, keys, val1, val2, val3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data is dense.\n",
    "* The search is linear.\n",
    "* Once we get to size of list or 2 or more, the database way is worss than modern dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_linear_search():\n",
    "    pprint(list(zip(keys, val1, val2, val3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('guido', 'blue', 'austin', 'apple'),\n",
      " ('sarah', 'orange', 'dallas', 'banana'),\n",
      " ('barry', 'green', 'tuscon', 'orange'),\n",
      " ('rachel', 'yellow', 'reno', 'pear'),\n",
      " ('tim', 'red', 'portland', 'peach')]\n"
     ]
    }
   ],
   "source": [
    "database_linear_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LISP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Store lists of pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_lists():\n",
    "    pprint([\n",
    "        list(zip(keys, val1)),\n",
    "        list(zip(keys, val2)),\n",
    "        list(zip(keys, val3)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('guido', 'blue'),\n",
      "  ('sarah', 'orange'),\n",
      "  ('barry', 'green'),\n",
      "  ('rachel', 'yellow'),\n",
      "  ('tim', 'red')],\n",
      " [('guido', 'austin'),\n",
      "  ('sarah', 'dallas'),\n",
      "  ('barry', 'tuscon'),\n",
      "  ('rachel', 'reno'),\n",
      "  ('tim', 'portland')],\n",
      " [('guido', 'apple'),\n",
      "  ('sarah', 'banana'),\n",
      "  ('barry', 'orange'),\n",
      "  ('rachel', 'pear'),\n",
      "  ('tim', 'peach')]]\n"
     ]
    }
   ],
   "source": [
    "association_lists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use multiple \"buckets\" to reduce the linear search by a constant factor.\n",
    "* The higher the number of buckets is, the lower look-up time is.\n",
    "* As the size of dictionary gets bigger, the performance decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_chaining(n):\n",
    "    buckets = [[] for _ in range(n)]\n",
    "    for pair in entries:\n",
    "        hashvalue, key, value = pair\n",
    "        i = hashvalue % n\n",
    "        buckets[i].append(pair)\n",
    "#     pprint(buckets)\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1693333087535225058, 'guido', 'blue'),\n",
       "  (1289971815486709344, 'barry', 'green'),\n",
       "  (3534391722751367630, 'rachel', 'yellow'),\n",
       "  (8468031612747961020, 'tim', 'red')],\n",
       " [(357032982473426601, 'sarah', 'orange')]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate_chaining(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1289971815486709344, 'barry', 'green'),\n",
       "  (8468031612747961020, 'tim', 'red')],\n",
       " [(357032982473426601, 'sarah', 'orange')],\n",
       " [(1693333087535225058, 'guido', 'blue'),\n",
       "  (3534391722751367630, 'rachel', 'yellow')],\n",
       " []]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate_chaining(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Periodically resize the dictionary so that it will never get more than 2/3 full.\n",
    "* Faster resizing store hash values in the list so that it could avoid calculating them again, which is very expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faster_resize(old_buckets, n):\n",
    "    n *= 2\n",
    "    new_buckets = [[] for _ in range(n)]\n",
    "    for old_bucket in old_buckets:\n",
    "        if not old_bucket:\n",
    "            continue\n",
    "        \n",
    "        for hashvalue, key, val in old_bucket:\n",
    "            bucket = new_buckets[hashvalue % n].append((hashvalue, key, val))\n",
    "    pprint(new_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1289971815486709344, 'barry', 'green')],\n",
      " [(357032982473426601, 'sarah', 'orange')],\n",
      " [(1693333087535225058, 'guido', 'blue')],\n",
      " [],\n",
      " [(8468031612747961020, 'tim', 'red')],\n",
      " [],\n",
      " [(3534391722751367630, 'rachel', 'yellow')],\n",
      " []]\n"
     ]
    }
   ],
   "source": [
    "faster_resize(separate_chaining(4), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When searching a bucket, we need to know whether the target key is found. We could test whether `key == target_key` but that can be slow because any object can define a complex or interesting `__eu__()` method.  \n",
    "The solution is to have two fast early-outs that can bypass equality testing in some circumstances.  \n",
    "* if two variables point to the same object, they are equal. \"identiy implies equality\"\n",
    "* For hash tables to work at all, they follow the invariant, \"if two objects are equal, then their hash values as equal as well.\" The contra-positive statement is \"if two objects have unequal hashes, then the objects must be unequal as well\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_match(key, target_key):\n",
    "    if key is target_key:\n",
    "        return True\n",
    "    if hash(key) != hash(target_key):\n",
    "        return False\n",
    "    return key == target_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Addressing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with separate chaining is that a great deal of space is wasted by having pointers to many separate lists.  \n",
    "The solution is to make the table more dense and to cope with collisions using linear probing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_addressing_linear(n):\n",
    "    table = [None] * n\n",
    "    for hashvalue, key, value in entries:\n",
    "        i = hashvalue % n\n",
    "        while table[i] is not None:\n",
    "            i = (i + 1) % n\n",
    "        table[i] = (hashvalue, key, value)\n",
    "    pprint(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3534391722751367630, 'rachel', 'yellow'),\n",
      " (357032982473426601, 'sarah', 'orange'),\n",
      " (8468031612747961020, 'tim', 'red'),\n",
      " (1693333087535225058, 'guido', 'blue'),\n",
      " (1289971815486709344, 'barry', 'green')]\n"
     ]
    }
   ],
   "source": [
    "open_addressing_linear(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleted Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that removing a key leaves a \"hole\" so that linear probing isunable to find keys that had collisions.  \n",
    "<br><br>\n",
    "The solution is to mark the slot with as `DUMMY` entry to serve as a placeholder. During the key-insertion phase, we try to re-use that dummy entry whenever possible.  \n",
    "Thei slookup scheme is known as \"Knuth - Algorithm D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(h, key):\n",
    "    freeslot = None\n",
    "    i = h % n\n",
    "    while True:\n",
    "        entry = table[i]\n",
    "        if entry == FREE:\n",
    "            return entry if freeslot is None else table[freeslot]\n",
    "        elif entry == DUMMY:\n",
    "            if freeslot is None:\n",
    "                freeslot = i\n",
    "        elif fast_match(key, entry.key):\n",
    "            return entry\n",
    "        i = (i + 1) % n\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with linear porbing is that we can end up with catastrophic linear pile-up.  \n",
    "<br><br>\n",
    "The solution is \"re-hash\" to other locations based on both the full values (all 64 bits) and on the number of probes.\n",
    "<br><br>\n",
    "* To make sure that probe sequence eventurally visite every possible slot, we use a simple linear_congruential random number generator thta provably eventually visits each slot, `i = 5 * i +1`\n",
    "* To make sure we use all the bits of the hash, we gradually shift in 5 bits at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_addressing_multihash(n):\n",
    "    table = [None] * n\n",
    "    for hashvalue, key, value in entries:\n",
    "        perturb = hashvalue\n",
    "        i = hashvalue % n\n",
    "        while table[i] is not None:\n",
    "            print('{} collided with {}'.format(key, table[i][1]))\n",
    "            i = (5 * i + perturb + 1) % n\n",
    "            perturb >>= 5\n",
    "        table[i] = (hashvalue, key, value)\n",
    "    pprint(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sarah collided with guido\n",
      "[None,\n",
      " None,\n",
      " (357032982473426601, 'sarah', 'orange'),\n",
      " (1289971815486709344, 'barry', 'green'),\n",
      " (8468031612747961020, 'tim', 'red'),\n",
      " (3534391722751367630, 'rachel', 'yellow'),\n",
      " (1693333087535225058, 'guido', 'blue')]\n"
     ]
    }
   ],
   "source": [
    "open_addressing_multihash(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early-Out For Lookups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the core of Python relies heavily on dict lookups but many times that same lookup must be repeated because the dictionary has mutated.  \n",
    "<br><br>\n",
    "The solution was created by Victor Stinner in \"PEP 509 - Add a private version to dict\". The idea is to update an interanl dict version number every time a dictionary is updated. The lets us do a fast version check to avoid slower repeated lookups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compact Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that dict tables have a lot of empty space interanlly for every entry which includes a hash, key and value.\n",
    "<br><br>\n",
    "The solution is to store the hash/key/value table densely and make a separate, tiny spared table of indices to vector into the dense table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compact_and_ordered(n):\n",
    "    table = [None] * n\n",
    "    for pos, entry in enumerate(entries):\n",
    "        h = perturb = entry[0]\n",
    "        i = h % n\n",
    "        while table[i] is not None:\n",
    "            i = (5 * i + perturb + 1) % n\n",
    "            perturb >>= 5\n",
    "        table[i] = pos\n",
    "    pprint(entries)\n",
    "    pprint(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1693333087535225058, 'guido', 'blue'),\n",
      " (357032982473426601, 'sarah', 'orange'),\n",
      " (1289971815486709344, 'barry', 'green'),\n",
      " (3534391722751367630, 'rachel', 'yellow'),\n",
      " (8468031612747961020, 'tim', 'red')]\n",
      "[2, 1, 0, None, 4, None, 3, None]\n"
     ]
    }
   ],
   "source": [
    "compact_and_ordered(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key-Sharing Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with previous design is if you have several dictionaries with the same keys, then there is unneccessary repetition os the keys, hash values and indices.\n",
    "<br><br>\n",
    "The solution was proposed by Mark Shannon in \"PEP 412 - Key-Sharing Dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_and_compact(n):\n",
    "    table = [None] * n\n",
    "    for pos, entry in enumerate(comb_entries):\n",
    "        h = perturb = entry[0]\n",
    "        i = h % n\n",
    "        while table[i] is not None:\n",
    "            i = (5 * i + perturb + 1) % n\n",
    "            perturb >>= 5\n",
    "        table[i] = pos\n",
    "    pprint(entries)\n",
    "    pprint(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1693333087535225058, 'guido', 'blue'),\n",
      " (357032982473426601, 'sarah', 'orange'),\n",
      " (1289971815486709344, 'barry', 'green'),\n",
      " (3534391722751367630, 'rachel', 'yellow'),\n",
      " (8468031612747961020, 'tim', 'red')]\n",
      "[2, 1, 0, None, 4, None, 3, None]\n"
     ]
    }
   ],
   "source": [
    "shared_and_compact(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Future Has Density and Great Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the dict more sparse without moving any of the has/key/value entries. The additional sparsity only costs 8 bytes and removes *all* hash collisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds and Ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sets use a differnt strategy, a mix of multiple chaining and linear probing.\n",
    "* Cuckoo hashing is still possible with the current desing though it is likely not going to be a win.\n",
    "* SipHash is used for strings to prevent deliberate collisions.\n",
    "* Internally, dict and sets have additional logic to guard against nutation while iterating."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
